{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table><tr><td><img src=\"images/dbmi_logo.png\" width=\"75\" height=\"73\" alt=\"Pitt Biomedical Informatics logo\"></td><td><img src=\"images/pitt_logo.png\" width=\"75\" height=\"75\" alt=\"University of Pittsburgh logo\"></td></tr></table>\n",
    " \n",
    " \n",
    " # Social Media and Data Science - Part 5\n",
    " \n",
    " \n",
    "Data science modules developed by the University of Pittsburgh Biomedical Informatics Training Program with the support of the National Library of Medicine data science supplement to the University of Pittsburgh (Grant # T15LM007059-30S1). \n",
    "\n",
    "Developed by Harry Hochheiser, harryh@pitt.edu. All errors are my responsibility.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as smoking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final part of our journey through social media data retrieval, annotation, natural langauge processing, and classififcation will challenge you to apply these techniques to a new problem. Specifically, you will create, annotate, and process a new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0.1 Setup\n",
    "\n",
    "As before, we start with the Tweets class and the configuration for our Twitter API connection.  We may not need this, but we'll load it in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",tweet_mode='extended',count=corpus_size)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(30)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def combineTweets(self,other):\n",
    "        for otherid in other.getIds():\n",
    "            tweet = other.getTweet(otherid)\n",
    "            searchTerm = other.getSearchTerm(otherid)\n",
    "            searchTime = other.getSearchTime(otherid)\n",
    "            self.addTweet(tweet,searchTime,searchTerm)\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' in tweet:\n",
    "            return tweet['codes']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the values of your keys into these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load some routines that we defined in [Part 3](SocialMedia - Part 3.ipynb):\n",
    "    \n",
    "1. Our routine for creating a customized NLP pipeline\n",
    "2. Our routine for including tokens\n",
    "3. The `filterTweetTokens` routine defined in an exercise (Without the inclusion of named entities. It will be easier to leave them out for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp\n",
    "\n",
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val\n",
    "\n",
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will include some additional modules from Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to go along for an exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the source of social media comments might be an important step in the process of interpreting a large corpus. Continuing with our example of smoking and vaping, it might be interesting to compare tweets from users - people who are talking about their own personal use  to those who might be either promoting vaping  (manufacturers, sponsors, etc.) or warning about dangers of vaping (physicians, researchers, public health agencies, etc.).\n",
    "\n",
    "A team of researchers at RTI International tackled this problem in a 2018 paper [Classification of Twitter Users Who Tweet About E-Cigarettes](http://publichealth.jmir.org/2017/3/e63/) by Annice Kim and colleagues collected tweets and attributed them to individuals, enthusiasts, \"informed agencies (news media or health community), marketers, or spammers. \n",
    "\n",
    "Your goal here is to collect a small data set and to attempt a smaller version of this challenge. Specifically, we will try to collect preliminary data for a classifier capable of identifing tweets from users of e-cigarettes vs. others.  Using any of the code found in Parts 1-4, complete these steps:\n",
    "\n",
    "1. Run some searches for tweets like 'e-cig', 'e-cigarette', 'vape' and 'vaping'. Collect a corpus of 200-300  or more tweets. You might want to save each of these result sets in files.\n",
    "\n",
    "2. Combine these tweets into one large collection using the 'Tweet' class listed above. Save the results in a file \n",
    "\n",
    "3. Annotate 50 of these tweets as pertaining to either 'individual' or 'non-individual'. Be sure that you do at least a few of the tweets from each of the original sets. One way to do this might be to randomize the tweets. Save the annotated results in a file. \n",
    "\n",
    "4.Review at the distrbution. Is it close to even? If not, do more.\n",
    "\n",
    "5. Take your annotated tweets - split them into train (80%) and test (20%) sets.  Process the train data and build a model (based on a TfIdf Vectorizer and an SVM). Evaluate the model on the test data sets.\n",
    "\n",
    "6. Test your model on the remaining tweets. What does your result look like?\n",
    "\n",
    "7. Review some of the data to identify opportunities for improvement - how might you make these models bettter?\n",
    "\n",
    "8. Reflect on the reproducibility and the reusability of the code: what should be done to make these tools easier to apply to other datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "*ANSWER FOLLOWS - insert answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run some searches for tweets like 'e-cig', 'e-cigarette', 'vape' and 'vaping'. Collect a corpus of 200-300 or more tweets. You might want to save each of these result sets in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = Tweets(\"e-cig\", 100)\n",
    "tweets1.saveTweets('part5_e-cig.json')\n",
    "tweets2 = Tweets(\"e-cigarette\", 100)\n",
    "tweets2.saveTweets('part5_e-cigarette.json')\n",
    "tweets3 = Tweets(\"vape\", 100)\n",
    "tweets3.saveTweets('part5_vape.json')\n",
    "tweets4 = Tweets(\"vaping\", 100)\n",
    "tweets4.saveTweets('part5_vaping.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Combine these tweets into one large collection using the 'Tweet' class listed above. Save the results in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.combineTweets(tweets2)\n",
    "tweets1.combineTweets(tweets3)\n",
    "tweets1.combineTweets(tweets4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.countTweets()\n",
    "tweets1.saveTweets('part5_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Annotate 50 of these tweets as pertaining to either 'individual' or 'non-individual'. Be sure that you do at least a few of the tweets from each of the original sets. One way to do this might be to randomize the tweets. Save the annotated results in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = Tweets()\n",
    "tweets1.readTweets('part5_tweets.json')\n",
    "tweet_ids = []\n",
    "for i in range(50):\n",
    "    tweet_id = random.choice(list(tweets1.getIds()))\n",
    "    tweet_ids.append(tweet_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "#TheEastLight Lee Seokcheol \"members were even chocked by guitar string, and forced to smoke e-cigarette despite knowing we are minors. they even put the cigarette right to our mouth. Now we even received psychology treatment because it was traumatic\" https://t.co/53msdzcbWu https://t.co/aEDiQfkoq3\n",
      "2\n",
      "We hope everyone is enjoying the last day of the #CWCBExpo! Don't miss the last chance to talk with our sales director Jackie(781-420-4127) at booth #507! ðŸ‘ˆ#vape #vapepen #vaporizer #vapecarts #vapeindustry #vapenation #cbd #cbdoil #cbdvape #cbdpen #cannabis #extracts #marijuana https://t.co/CIrkATzI7v\n",
      "3\n",
      "Dude just cruised past me riding a lawn chair taped to an electric skateboard while vaping and blasting Jack Johnson. Now Iâ€™m questioning all my life choices. https://t.co/VfFlJZKil4\n",
      "4\n",
      "Arizona to focus on putting out e-cigarette use among youths https://t.co/ffcO1QexWW\n",
      "5\n",
      "E-cigarette retail sales have surged in recent years, according to new data, and a large portion are targeting teen and preteen markets https://t.co/wV88VrLueM by @drninashapiro https://t.co/4uSh2rAsU5\n",
      "6\n",
      "People really thinking smoking and vaping are personality traits\n",
      "7\n",
      "If you thought smoking an e-cigarette would be safer than smoking a traditional cigarette, this will make you think again...\n",
      "\n",
      "https://t.co/NrJ3IkHD81\n",
      "8\n",
      "Juul's have seen a massive popularity boom this year, but scientists are now learning the side effects could be much worse than you think. #JMUMktg410\n",
      "\n",
      "https://t.co/8uPXecdJ31\n",
      "9\n",
      "No e-cig almost at week one.\n",
      "I kind of donâ€™t care about updating you guys on this because it wasnâ€™t that hard. I instead send my prayers out to those struggling with more addictive substances. You can do it, fuck the man https://t.co/Rl1Ouwx9y3\n",
      "10\n",
      "Over half of e-cigarette users in England have stopped smoking completely and of the 45% who still smoke, half are vaping in order to stop smoking. Find out more about e-cigarettes and other smoking quitting routes in #PHEHealthMatters: https://t.co/52fEbdHhuU https://t.co/0qDkeiJvJs\n",
      "11\n",
      "E-cigarette sales could be confined to vape shops, Commissioner Scott Gottlieb tells CNBC. https://t.co/QLDThRC3A5\n",
      "12\n",
      "E-cigarette use is on the rise in St. Clair County teens! Talk to your teens about the dangers of e-cigarettes and all tobacco products. https://t.co/gomlq67yde\n",
      "13\n",
      "Over half of e-cigarette users in England have stopped smoking completely and of the 45% who still smoke, half are vaping in order to stop smoking. Find out more about e-cigarettes and other smoking quitting routes in #PHEHealthMatters: https://t.co/52fEbdHhuU https://t.co/0qDkeiJvJs\n",
      "14\n",
      "@therealdrix78 I vape and make my own vape juice so that I basically know what's in it ðŸ˜˜ðŸ’– I've been a smoker since 1999 until I MOSTLY quit almost 2 years ago. Now I smoke once in a great while. https://t.co/LVPzkoerwv\n",
      "15\n",
      "I liked a @YouTube video https://t.co/2iTXQnI2yR Not Another Vape Show 166 - T.G.I.S\n",
      "16\n",
      "@mackenzie_ik itâ€™s like an e-cigarette\n",
      "17\n",
      "@chinamyrick It's like the Four Loko of vaping\n",
      "18\n",
      "@smuttyparadaisu @eyemansally Itâ€™s a Juul, itâ€™s an e-cigarette meant to replace or curb the use of cigarettes, but people tend to buy them just to have one due to the high nicotine concentration in each pod (each Juul pod is roughly 1 pack of cigs).\n",
      "19\n",
      "@StevieBloke I had a lungful of e-cig too which is why I choked at the end. At least I made the bitch say please. On a worse day Iâ€™d have sat there.\n",
      "20\n",
      "@okayhotshot i investigated in the thread, it's one of those disposable e-cig or something\n",
      "21\n",
      "@MarijuanaPosts Ask politely...May I please have a Cookies Vape Pen by Sublime. Thank you kindly.\n",
      "22\n",
      "@BonjourDev It's a lil vape stick, real slick looking and you can get all these flavors, it has a good % of nicotine at I think 5% strength in their \"pods\" which hold the juice. I'd consider it more of an e-cig if anything. But it's cool, they are sold at gas stations and shit\n",
      "23\n",
      "Dude just cruised past me riding a lawn chair taped to an electric skateboard while vaping and blasting Jack Johnson. Now Iâ€™m questioning all my life choices. https://t.co/VfFlJZKil4\n",
      "24\n",
      "Thatâ€™s an e-cig https://t.co/G9YN0drx67\n",
      "25\n",
      "â€˜I vapeâ€™ \n",
      "\n",
      "-men who wear womenâ€™s lingerie\n",
      "26\n",
      "v6 camaro the offical car of fathers that vape and smoke at the same time\n",
      "27\n",
      "Get a smoother vape &amp; better taste! Nic Salts Forest Fruit #eJuice by Airship A wonderful balanced blend of sweet and tart flavors of many berries https://t.co/c78xXt3g7d #vape #ecig #vaping #vapelife #vapefam #eliquid #startvaping #vapor #vapefriends #vapenation #vapeon .@Cuecig https://t.co/afJGnHhnou\n",
      "28\n",
      "\"Big Tobacco Might Get Burned by Hong Kong's E-Cig Ban\" via FOX BIZ https://t.co/UNohhPJIr6 https://t.co/ocF9c9Pnfy\n",
      "29\n",
      "Unique Air Intake System\n",
      "The More Intake - The More Output\n",
      "Serenity 400mAH v v Battery + BCC-2 Cartridge\n",
      "Variable Voltage with Even Core Heat Distribution Cartridge\n",
      "Powerful Punch and Lots of Torque\n",
      "Because Output Counts\n",
      "https://t.co/ujcoYnADZF https://t.co/THZrPU4n3x\n",
      "30\n",
      "Greg has been partaking in a new phenomenon known as \"vaping\". From what I know, it's basically meth for fucking losers. Be a man, Gregory!\n",
      "31\n",
      "Dude just cruised past me riding a lawn chair taped to an electric skateboard while vaping and blasting Jack Johnson. Now Iâ€™m questioning all my life choices. https://t.co/VfFlJZKil4\n",
      "32\n",
      "Monday! Back to work with #MFENG\n",
      "\n",
      "available online https://t.co/vkAC5BeWva\n",
      "wholesale inquiries: sales@snowwolfvape.com\n",
      "for more info check: https://t.co/wHTXdRD2iH\n",
      "#Vapeporn #Calivapers #Vapelife #Vapelyfe #Vapestagram #Vapedaily #Vape #Vaper #Vapers #Vapor #Vaporwave https://t.co/hVeyno0Wta\n",
      "33\n",
      "March 2015\n",
      "\n",
      "A angry w/o reason, cover the cctv &amp; window &amp; hit member around 20 times w/ baseball bat\n",
      "\n",
      "Kim Chang Hwan (CEO) receive e-cigarette as gift. #LeeSeungHyun who's in middle school was forced to smoke even tho he's not smoking https://t.co/3KYSp7P97J #KoreanUpdates VF https://t.co/vbTcOvyWX7\n",
      "34\n",
      "Increasing E-Cigarette Sales to Minors Prompts FDA to Crack Down https://t.co/q4fQm0e5VZ https://t.co/5HjczI8RtR\n",
      "35\n",
      "Bar &amp; Club Stats has a better way to check IDs and become smarter about your customers.\n",
      "\n",
      "Protect your bar, #brewery, bottle shop, restaurant, event, casino, stadium, #marijuana dispensary, #vape shop or liquor store.\n",
      "\n",
      "Download it now FREE!\n",
      "\n",
      "#beer #craftbeer #brewing #cannabis #ad\n",
      "36\n",
      "Get yourÂ #coils, ecig tanks &amp;Â #ClearomizersÂ at great prices! Large selection!  https://t.co/cjPjuWBPwLÂ #ecigÂ #vapeÂ #vaping #vapecloud #vapelife #vapefam #ejuice #replacementcoils #ecigcoils #vapecoils #bestvape #bestecig #buycig #stopsmoking .@Cuecig https://t.co/DGhIqBNkyA\n",
      "37\n",
      "my ninja, when I tell you how clutch these vape pens are. diablo. I was gettin lifted in the movie theatre watchin this boy Michael Myers get BUSY\n",
      "38\n",
      "Iâ€™m not a girl but this is definitely a JUUL (e-cig) .. give me that $100 https://t.co/uAdyhp5JrW\n",
      "39\n",
      "I hope @erbrod listens to the mouse. He seems to know more about this subject than she does!\n",
      "#vaping #ecigs https://t.co/ORexFMAJDr\n",
      "40\n",
      "Is e-cigarette #vaping associated with random pattern flap viability in rats as is traditional cigarette smoking? Read more in this investigation from researchers at @BUMedicine https://t.co/ICPSKScRA5\n",
      "41\n",
      "Even better news on #vaping: A packed programme at the E-Cigarette Summit in London this November https://t.co/VFGZRABjau\n",
      "42\n",
      "What's that word for having toffee sauce on your e-cig and none on your ice cream? https://t.co/LrEjNYDxgJ\n",
      "43\n",
      "The Thai Authorities are Considering Legalizing E-cigaretteÂ Imports https://t.co/8eQYEcgHbU https://t.co/vmaYmk1W8K\n",
      "44\n",
      "We are sorry to let you know that the E-CIg Lounge has closed! Please come visit us at Smith Gallery for local art! https://t.co/Wx0pBiHb7v\n",
      "45\n",
      "@LaughingLatina any vaping or smoking one\n",
      "46\n",
      "Go vape! Many individuals nonetheless consider that vaping is simply as dangerous as smoking odd cigarettes â€“ nevertheless itÂ isnâ€™t https://t.co/zKy1Qfm6z2 https://t.co/Fvtba9xtrH\n",
      "47\n",
      "March 2015 in Media Line's 5th floor studio\n",
      "\n",
      "CEO of Media Line, Kim Changhwan forces Lee Seunghyun, who is currently a middle schooler, to smoke an e-cigarette that he receives as a gift\n",
      "48\n",
      "How #eCigs Led Me to Starting My Own #Vaping Business\n",
      "https://t.co/QGC9kGzfTv https://t.co/9n138VSSNR\n",
      "49\n",
      "The e-cig arm of Imperial tobacco @FontemVentures says \"Fontem Ventures' transformative purpose is to go from tobacco to something better through leadership in vaping technology'\n",
      "50\n",
      "Over half of e-cigarette users in England have stopped smoking completely and of the 45% who still smoke, half are vaping in order to stop smoking. Find out more about e-cigarettes and other smoking quitting routes in #PHEHealthMatters: https://t.co/52fEbdHhuU https://t.co/0qDkeiJvJs\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for tweet_id in tweet_ids:\n",
    "    print(i)\n",
    "    print(tweets1.getText(tweet_id))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[0], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[1], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[2], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[3], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[4], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[5], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[6], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[7], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[8], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[9], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[10], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[11], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[12], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[13], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[14], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[15], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[16], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[17], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[18], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[19], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[20], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[21], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[22], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[23], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[24], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[25], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[26], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[27], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[28], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[29], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[30], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[31], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[32], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[33], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[34], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[35], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[36], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[37], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[38], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[39], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[40], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[41], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[42], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[43], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[44], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[45], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[46], 'NON-INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[47], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[48], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1.addCode(tweet_ids[49], 'INDIVIDUAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineTweetsIds(original,other):\n",
    "    for otherid in other:\n",
    "        tweet = tweets1.getTweet(otherid)\n",
    "        searchTerm = tweets1.getSearchTerm(otherid)\n",
    "        searchTime = tweets1.getSearchTime(otherid)\n",
    "        original.addTweet(tweet,searchTime,searchTerm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "#tweets1.saveTweets('part5_tweets_annotated.json')\n",
    "print(len(tweet_ids))\n",
    "tweets = Tweets()\n",
    "combineTweetsIds(tweets, tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.countTweets()\n",
    "tweets.saveTweets('part5_tweets_annotated.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Review at the distrbution. Is it close to even? If not, do more.\n",
    "\n",
    "Close to even distrubution so continuing with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NON-INDIVIDUAL', 21), ('INDIVIDUAL', 28)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.getCodeProfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 17 11 12\n"
     ]
    }
   ],
   "source": [
    "one = two = three = four = 0\n",
    "for tweet_id in tweet_ids:\n",
    "    term = tweets1.getSearchTerm(tweet_id)\n",
    "    if term == 'e-cig':\n",
    "        one += 1\n",
    "    elif term == 'e-cigarette':\n",
    "        two += 1\n",
    "    elif term == 'vape':\n",
    "        three +=1 \n",
    "    elif term == 'vaping':\n",
    "        four += 1\n",
    "print(one, two, three, four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Take your annotated tweets - split them into train (80%) and test (20%) sets. Process the train data and build a model (based on a TfIdf Vectorizer and an SVM). Evaluate the model on the test data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tokenizeText(text):\n",
    "    nlp=getTwitterNLP()\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens(tokens)\n",
    "\n",
    "vectorizer= TfidfVectorizer(tokenizer=tokenizeText,preprocessor=lambda x: x)\n",
    "clf = LinearSVC()\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenTweets(tweets):\n",
    "    flat=[]\n",
    "    for i in tweets.getIds():\n",
    "        text = tweets.getText(i)\n",
    "        for x in tweets.getCodes(i):\n",
    "            cat = x\n",
    "        pair =(text,cat)\n",
    "        flat.append(pair)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrainSplit(pairs,splitFactor=0.8):\n",
    "    random.shuffle(pairs)\n",
    "    split=int(len(pairs)*splitFactor)\n",
    "    train=pairs[:split]\n",
    "    test =pairs[split:]\n",
    "    return train,test\n",
    "\n",
    "def getTestTrain(tweets,splitFactor=0.8):\n",
    "    tweets = flattenTweets(tweets)\n",
    "    train,test=getTestTrainSplit(tweets,splitFactor)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 10\n"
     ]
    }
   ],
   "source": [
    "train,test=getTestTrain(tweets)\n",
    "print(str(len(train))+ \" \"+str(len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTexts,trainCats=zip(*train)\n",
    "testTexts,testCats=zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('#TheEastLight Lee Seokcheol \"members were even chocked by guitar string, and forced to smoke e-cigarette despite knowing we are minors. they even put the cigarette right to our mouth. Now we even received psychology treatment because it was traumatic\" https://t.co/53msdzcbWu https://t.co/aEDiQfkoq3',\n",
       " 'INDIVIDUAL')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(trainTexts,trainCats)\n",
    "preds = pipe.predict(testTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Test your model on the remaining tweets. What does your result look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToNumeric(cats):\n",
    "    nums =[]\n",
    "    for c in cats:\n",
    "        if c =='INDIVIDUAL':\n",
    "            nums.append(1)\n",
    "        elif c=='NON-INDIVIDUAL':\n",
    "            nums.append(-1)\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCats=convertToNumeric(testCats)\n",
    "numPreds=convertToNumeric(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is [1.    0.375]\n",
      "Recall is [0.28571429 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Precision is \"+str(precision_score(numCats,numPreds,average=None)))\n",
    "print(\"Recall is \"+ str(recall_score(numCats,numPreds,average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Review some of the data to identify opportunities for improvement - how might you make these models bettter?\n",
    "\n",
    "These models can be made better by identifying features specific to the terms being analyzed and the tweets related to those terms. Once the discriminating features are known, the models can be trained better using those features. Such features may be counting terms related to smoking, certain hashtags/emoticons always found with vaping tweets, style of tweeting by individuals, presence of URL's common in non-individual tweets, individuals using more hashtags etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Reflect on the reproducibility and the reusability of the code: what should be done to make these tools easier to apply to other datasets.\n",
    "\n",
    "The Tweets class makes a very good attempt at making the code reproducible and reusable. The NLP routine getTwitterNLP() can be made more generalized so as to be applicable to other datasets (with few modifications if needed). Currently special cases are being hard coded, these can be recoded. Moreover, the flattenTweets() routine only considers annotation using the search term whereas above we have used the codes, the routine can be modified such that this is taken into consideration.\n",
    "\n",
    "If datasets used are not taken from Twitter, and instead from other social media sites such as Reddit or YouTube then the code may not work very well. The routines would have to be rewritten depending on the specific characteristics of multiple datasets to make it reproducible and reusable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
